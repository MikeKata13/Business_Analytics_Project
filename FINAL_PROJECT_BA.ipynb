{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlh1ucXFYRMX"
      },
      "source": [
        "# Introduction to Business Analytics\n",
        "## Bike sharing trips in New York\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzStKYbKYRMb"
      },
      "source": [
        "Participants:\n",
        "\n",
        "- Elli Georgiou: s223408\n",
        "- Maria Katarachia: s213633\n",
        "- Stavroula Douva: s222652\n",
        "- Michail-Achillefs Katarachias: s222653\n",
        "- Dimitris Voukatas: s230148\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKokbIHcYRMc"
      },
      "source": [
        "### Table of Content\n",
        "- Section 1: **Introduction + Data Analysis \\& Visualizations**\n",
        "- Section 2: **Prediction models**\n",
        "- Section 3: **Exploratory Component**\n",
        "- Section 4: **Conclusions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HNv1HdSoYRMd"
      },
      "source": [
        "### Section 1 : Introduction and Data Analysis and Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66HrbHIrYRMe"
      },
      "source": [
        "Firstly, we import all the libraries that will be used in this project."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YOiN-lJRYRMf"
      },
      "outputs": [],
      "source": [
        "%reset -f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "baT0P1qGYRMh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import plotly.express as px\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from time import time\n",
        "import tqdm\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import r2_score\n",
        "from datetime import datetime\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
        "import plotly.graph_objects as go\n",
        "import math\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDm3CFHPYRMi"
      },
      "source": [
        "#### Data importing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IcJYT-1XYRMj"
      },
      "outputs": [],
      "source": [
        "file_path = 'Trips_2018.csv'\n",
        "\n",
        "if not os.path.exists('plots'):\n",
        "    os.makedirs('plots')\n",
        "\n",
        "with open(file_path, 'r') as f:\n",
        "    df = pd.read_csv(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeiY4b5bYRMk"
      },
      "source": [
        "#### Data preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjtK2jHiYRMl"
      },
      "source": [
        "Let's take a look at how our dataframe looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQxkqbZ0YRMl",
        "outputId": "ad1e63c7-d60e-4ac1-ce6e-729fd3f59c5e"
      },
      "outputs": [],
      "source": [
        "df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ESe3KFsPYRMn"
      },
      "outputs": [],
      "source": [
        "# We remove the first column as it is just an intex and does not contribute to our analysis\n",
        "df = df.drop(df.columns[0], axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhr5wig1YRMo",
        "outputId": "ddbb516a-f309-4a9f-d9fc-1748ed88b721"
      },
      "outputs": [],
      "source": [
        "# Handling missing values\n",
        "df.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U02aSgEUYRMp",
        "outputId": "3b05bddd-802d-4540-cced-600c76805ab1"
      },
      "outputs": [],
      "source": [
        "df.duplicated().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7ifonOPPYRMq"
      },
      "outputs": [],
      "source": [
        "df = df[pd.to_numeric(df['start_station_id'], errors='coerce').notnull()]\n",
        "df = df[pd.to_numeric(df['end_station_id'], errors='coerce').notnull()]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ScpkrYUYRM8"
      },
      "outputs": [],
      "source": [
        "df['starttime'] = pd.to_datetime(df['starttime'])\n",
        "df['stoptime'] = pd.to_datetime(df['stoptime'])\n",
        "df['date'] = df['starttime'].dt.date\n",
        "df['hour'] = df['starttime'].dt.hour\n",
        "df['day_of_week'] = df['starttime'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SHj5CxrhYRM-"
      },
      "outputs": [],
      "source": [
        "df = df[df['tripduration'] > 0]\n",
        "df = df[df['tripduration'] <= 86400] #we improved the dataset by removing trips with negative or unusually lengthy durations, especially those lasting more than 86400 seconds (1 day)\n",
        "df = df[df['tripduration'] <= df['tripduration'].quantile(.99)] #to focus on more usual user behavior, we excluded trips outside the 99th percentile of trip durations.\n",
        "df['tripduration'] = (df['stoptime'] - df['starttime']).dt.total_seconds()\n",
        "df = df[df['birth_year'] >= df['birth_year'].quantile(.01)] #We filtered away records with birth years in the lowest 1st percentile to prevent outliers\n",
        "df = df[df['gender'].isin([0, 1, 2])]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JNuLg9d1YRM-",
        "outputId": "7aef2f83-abaa-46f8-cc94-3ac597d8353a"
      },
      "outputs": [],
      "source": [
        "df.describe() #we use df.describe to investigate more how our data behave"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVcUuzqAYRM_"
      },
      "source": [
        "#### Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OA_o98bUYRNA",
        "outputId": "6f8a5160-7162-4581-e45d-fff0cc6da46e"
      },
      "outputs": [],
      "source": [
        "# We visualize our data to gain useful insights.\n",
        "#  Distribution of trip durations\n",
        "\n",
        "fig = px.histogram(df, x='tripduration', nbins=100, title='Distribution of Trip Durations')\n",
        "fig.update_xaxes(title='Trip Duration (seconds)')\n",
        "fig.update_yaxes(title='Frequency')\n",
        "fig.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnOHAavEYRNA"
      },
      "source": [
        "The histogram above depicts the distribution of trip durations, in seconds. As the distribution is right skweed, we can tell that shorter trips are more frequent than longer ones. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUy3v3WPYRNB",
        "outputId": "df1cd6ba-10ef-48ef-a1f0-c4d1bfccd26d"
      },
      "outputs": [],
      "source": [
        "# Age distribution of the users.\n",
        "\n",
        "fig = px.histogram(df, x=2019 - df['birth_year'], nbins=100, title='Distribution of User Ages')\n",
        "fig.update_layout(xaxis_title='Age', yaxis_title='Frequency', showlegend=False)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC8Bo0_0YRNB"
      },
      "source": [
        "The age distribution histogram clearly indicates the majority of users are between the ages of 25 and 40. Surprisingly, there is an a rise around the age of 50 whichÂ could be due to a number of factors, including a default entry value, a data error, or a large number of people in that age range. We notice a decrease of the distribution for the younger (20 years) and older (>60 years) age groups, reflecting that these groups use the service less."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YCSrzregYRNC",
        "outputId": "302ca0ad-6e4d-4a03-fa08-6d587015f693"
      },
      "outputs": [],
      "source": [
        "gender_counts = df['gender'].value_counts()\n",
        "gender_counts\n",
        "labels = ['Male', 'Female', 'Unknown']\n",
        "colors = ['blue', 'pink', 'gray']\n",
        "\n",
        "fig = plt.figure(facecolor='white')\n",
        "plt.pie(gender_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
        "plt.axis('equal')  \n",
        "plt.title('Gender Distribution')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KW-zbPyYYRND"
      },
      "source": [
        "The pie chart depicts the gender distribution among our users. Men constitute close to 70% of the population, while women represent about 23%. On the other hand, 8% of the user base is labeled 'Unknown.' This could imply that consumers prefer not to identify their gender, or that gender data was not collected."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5TvxtPXcYRND"
      },
      "source": [
        "We finished the first phase of data preparation successfully, leaving us withÂ a cleaned and improved dataset. We're now ready to move on to the next exciting phase: clustering our data by geographic location."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVaQDMNuYRNE"
      },
      "source": [
        "#### Clustering"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cMA5CiOhYRNE",
        "outputId": "8954edfb-052a-4d3a-a8c5-52d70953381a"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "scaled_df = scaler.fit_transform(df[['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'birth_year', 'tripduration']])\n",
        "scaled_df = pd.DataFrame(scaled_df, columns=['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'birth_year', 'tripduration'])\n",
        "\n",
        "pca = PCA(n_components=6)\n",
        "pca.fit(scaled_df)\n",
        "pca_df = pca.transform(scaled_df)\n",
        "pca_df = pd.DataFrame(pca.components_, columns=['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'birth_year', 'tripduration'])\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
        "plt.xlabel('Number of components')\n",
        "plt.ylabel('Cumulative explained variance')\n",
        "plt.savefig(os.path.join('plots', 'pca_cumulative_explained_variance.png'))\n",
        "plt.show()\n",
        "\n",
        "corr_matrix = scaled_df.corr()\n",
        "corr_matrix['tripduration'].sort_values(ascending=False)\n",
        "\n",
        "for i in range(len(pca_df)):\n",
        "    print(f\"\\nPrincipal Component {i + 1} Loadings:\")\n",
        "    print(pca_df.iloc[i].sort_values(ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qbf-Id9YRNF"
      },
      "source": [
        "Our Principal Component AnalysisÂ results show that geographic coordinates, particularly start and end station latitudes and longitudes, play an important role in the variance of our data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkwEbZgYRNF"
      },
      "source": [
        "#### Elbow method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELK9Uh2_YRNP"
      },
      "source": [
        "In order to identify the optimal number of clusters for the clustering algorithm we chose the elbow method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Izj3jRMRYRNP",
        "outputId": "dea9482a-8ce4-430b-e31f-cfe1ae688510"
      },
      "outputs": [],
      "source": [
        "#####it takes approximately 30 minutes to run so we run it once and then we commented out####\n",
        "\n",
        "# coordinates_sample = df[['start_station_latitude', 'start_station_longitude']]\n",
        "# scaler = StandardScaler()\n",
        "# coordinates_standardized = scaler.fit_transform(coordinates_sample)\n",
        "\n",
        "# distortions = []\n",
        "# K = range(1, 40)\n",
        "# for k in K:\n",
        "#     kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
        "#     kmeans.fit(coordinates_standardized)\n",
        "#     distortions.append(kmeans.inertia_)\n",
        "\n",
        "# plt.figure(figsize=(12, 6))\n",
        "# plt.plot(K, distortions, 'bx-')\n",
        "# plt.xlabel('Number of clusters (k)')\n",
        "# plt.ylabel('Distortion (Inertia)')\n",
        "# plt.title('Elbow Method for Optimal k with Sampled Data')\n",
        "# plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7PuTBRfYRNQ"
      },
      "source": [
        "The optimal number of clustersÂ appears to be around 5 , meaning that adding more clusters beyond this point does not greatly improve the fineness of our grouping. Therefore we chose to continue with the minimum clusters that we are allowed which are 20."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rSTfO0SiYRNR"
      },
      "source": [
        "Preprocessed our location data to handle the outliers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvVNzMRwYRNR"
      },
      "outputs": [],
      "source": [
        "coordinates_start = df[['start_station_latitude', 'start_station_longitude']]\n",
        "coordinates_end = df[['end_station_latitude', 'end_station_longitude']]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "coordinates_standardized = scaler.fit_transform(coordinates_start)\n",
        "\n",
        "kmeans = KMeans(n_clusters=20, random_state=42)\n",
        "kmeans.fit(coordinates_standardized)\n",
        "df['cluster'] = kmeans.predict(coordinates_standardized)\n",
        "\n",
        "kmeans = KMeans(n_clusters=20, init='k-means++', random_state=42).fit(coordinates_start)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "df['cluster'] = kmeans.predict(coordinates_start)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8Pn2ajtYRNS"
      },
      "source": [
        "We effectively categorized our dataset into 20 distinctÂ clusters by applying the K-means algorithm to our standardized start station coordinates. Each data point was assigned to a single cluster using this method. The clustering helps us identifyÂ useful information on station popularity and user behavior trends."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bmfL88xUYRNT"
      },
      "outputs": [],
      "source": [
        "#Plot the clusters on a map.\n",
        "\n",
        "def plot_stations_map(stations):\n",
        "    #First before plotting we have to deal with the outliers\n",
        "    #The latitude of New York City is approximately between 40.4774 and 45.01585, and the longitude is approximately between -79.76259 and -71.18507.\n",
        "\n",
        "    lon_min = -79.76259\n",
        "    lat_min = 40.4774\n",
        "    lon_max = -71.18507\n",
        "    lat_max = 45.01585\n",
        "\n",
        "    # Store the stations that are within the boundaries\n",
        "    stations = stations[\n",
        "        (stations['start_station_latitude'] > lat_min) &\n",
        "        (stations['start_station_latitude'] < lat_max) &\n",
        "        (stations['start_station_longitude'] > lon_min) &\n",
        "        (stations['start_station_longitude'] < lon_max)\n",
        "    ]\n",
        "\n",
        "    title = 'Citi Bike Stations in New York City'\n",
        "    fig = px.scatter_mapbox(\n",
        "        stations,\n",
        "        lat='start_station_latitude',\n",
        "        lon='start_station_longitude',\n",
        "        color='cluster',\n",
        "        mapbox_style='carto-positron',\n",
        "        zoom=9,\n",
        "        width=1000,\n",
        "        height=600\n",
        "    )\n",
        "    fig.update_layout(\n",
        "        title=dict(\n",
        "            text=title,\n",
        "            x=0.5,\n",
        "            xanchor='center',\n",
        "            font=dict(size=20)\n",
        "        )\n",
        "    )\n",
        "    fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXtqZkw_YRNU"
      },
      "outputs": [],
      "source": [
        "#Remove the locations that are more than 3 standard deviations from the center of the clusters.\n",
        "# Calculate the distance between each point and its cluster center\n",
        "distance = kmeans.transform(coordinates_start)\n",
        "min_distance = np.min(distance, axis=1) #we get the minimum distance for each point and its cluster index\n",
        "min_distance_cluster = np.argmin(distance, axis=1)\n",
        "threshold = 2*np.std(distance,axis=1)\n",
        "within_threshold = np.argwhere(min_distance < threshold).flatten()\n",
        "\n",
        "df = df.iloc[within_threshold]#we remove the points that are outside the threshold distance of a cluster center\n",
        "\n",
        "# Plot the stations with an underlying map of New York City.\n",
        "plot_stations_map(df.sample(10000)) # We plotted a sample of 10000 points to avoid the file being too large\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fHwp9BX1YRNV"
      },
      "source": [
        "After clusteringÂ the stations, we concentrated on making these groups more relevant by removing outliers,Â especially, stations that were more than two standard deviations out from their cluster centers. The finalÂ result is a map, with each station color-coded to its respective cluster. This mapÂ not only depicts the distribution of bike stations around the city, but it also highlights locations with high station density."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOQVWJbcYRNV"
      },
      "source": [
        "#### Cluster with the largest demand"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58RUwdylYRNW"
      },
      "source": [
        "After identifying clusters based on departure stations, the next step is to determine which cluster exhibits the largest demand in terms of the number of pickups."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 617
        },
        "id": "iqtJWsZZYRNW",
        "outputId": "133f0570-de45-4d09-fbb8-ee9dc20427a8"
      },
      "outputs": [],
      "source": [
        "# We work with the cluster with the largest demand.\n",
        "\n",
        "larg_cluster = df['cluster'].value_counts().index[0]\n",
        "\n",
        "df_cluster = df[df['cluster'] == larg_cluster]\n",
        "\n",
        "# We want to create two timeseries that will describe for hourly intervals the pick up and the dropoffs counts for the one cluster with the largest demand.\n",
        "\n",
        "df_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xMu6RBomYRNX"
      },
      "source": [
        "### Section 2: Prediction models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kIjLO_t-YRNY"
      },
      "outputs": [],
      "source": [
        "# Data preproccesing we keep only the ccolumns we need\n",
        "columns_to_remove = ['start_station_latitude', 'start_station_longitude', 'end_station_latitude','end_station_longitude','bikeid','usertype','birth_year','gender']\n",
        "df_cluster = df_cluster.drop(columns=columns_to_remove, axis=1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6tJa5wgYRNY",
        "outputId": "f259a2fc-6118-4da0-80a3-1453720965d0"
      },
      "outputs": [],
      "source": [
        "# We observe how the new dataframe looks like\n",
        "df_cluster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKAhiLeMYRNZ"
      },
      "source": [
        "#### Pick up  & Drop off Dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xEv3YeqhYRNa"
      },
      "source": [
        "Initially we create the pick ups dataframe. We have divided our data into hourly intervals in order to prepare it for analysis and make sure it was best organized for our assessment procedure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We applied time aggregation so we can forecast the demand on an hourly basis. By selecting the relevant columns only we made sure that our models focus on the most impactful features, which are the times of pickups and dropoffs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RuEFSYUYYRNa"
      },
      "outputs": [],
      "source": [
        "df_cluster['ride_count'] = 1\n",
        "hourly_pickups = df_cluster.groupby(pd.Grouper(key='starttime', freq='1H')).agg({'ride_count': 'sum',  }).reset_index()\n",
        "hourly_dropoff = df_cluster.groupby(pd.Grouper(key='stoptime', freq='1H')).agg({'ride_count': 'sum',  }).reset_index()\n",
        "\n",
        "hourly_dropoff = hourly_dropoff.rename(columns={'ride_count': 'dropoff_counts'})\n",
        "hourly_pickups = hourly_pickups.rename(columns={'ride_count': 'pickup_counts'})\n",
        "\n",
        "\n",
        "hourly_pickups.set_index('starttime', inplace=True)\n",
        "hourly_dropoff.set_index('stoptime', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGC3doIIYRNc"
      },
      "outputs": [],
      "source": [
        "# We plot the pick ups on hourly basis \n",
        "\n",
        "fig = px.line(hourly_pickups, x=hourly_pickups.index, y='pickup_counts', title='Hourly Pickup Counts Time Series')\n",
        "fig.update_layout(\n",
        "    xaxis_title='Datetime',\n",
        "    yaxis_title='Pickup Counts',\n",
        "    hovermode='x',  \n",
        "    template='plotly_dark'  \n",
        ")\n",
        "\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zickLEpeYRNc"
      },
      "source": [
        "As we expected from spring till late Autumn where the weather conditions are ideally to rent the bike and especially during the summer period the demand of bike sharing is at its peak."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EloXLuSaYRNd"
      },
      "outputs": [],
      "source": [
        "fig = px.line(hourly_dropoff, x=hourly_dropoff.index, y='dropoff_counts', title='Hourly Dropoff Counts Time Series')\n",
        "\n",
        "fig.update_layout(\n",
        "    xaxis_title='Datetime',\n",
        "    yaxis_title='Dropoff Counts',\n",
        "    hovermode='x',  \n",
        "    template='plotly_dark'  \n",
        ")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epiaVzSBYRNe"
      },
      "source": [
        "#### Linear Regression Model Fitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VImSvxL6YRNf"
      },
      "source": [
        "To handle the challenge of predicting bike-sharing demand, we used hourly time aggregation, enabling us to accurately forecast the number of pickups and dropoffs for each hour of the next day."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8CUQlls6YRNf"
      },
      "outputs": [],
      "source": [
        "hourly_pickups['lag_pickups_24hr'] = hourly_pickups['pickup_counts'].shift(24)\n",
        "hourly_dropoff['lag_dropoffs_24hr'] = hourly_dropoff['dropoff_counts'].shift(24)\n",
        "\n",
        "lag_hours_day = 48\n",
        "hourly_pickups['lag_pickups_2day'] = hourly_pickups['pickup_counts'].shift(lag_hours_day)\n",
        "hourly_dropoff['lag_dropoffs_2day'] = hourly_dropoff['dropoff_counts'].shift(lag_hours_day)\n",
        "\n",
        "lag_hours_day= 72\n",
        "hourly_pickups['lag_pickups_3day'] = hourly_pickups['pickup_counts'].shift(lag_hours_day)\n",
        "hourly_dropoff['lag_dropoffs_3day'] = hourly_dropoff['dropoff_counts'].shift(lag_hours_day)\n",
        "\n",
        "lag_hours_day= 96\n",
        "hourly_pickups['lag_pickups_4day'] = hourly_pickups['pickup_counts'].shift(lag_hours_day)\n",
        "hourly_dropoff['lag_dropoffs_4day'] = hourly_dropoff['dropoff_counts'].shift(lag_hours_day)\n",
        "\n",
        "lag_hours_day= 120\n",
        "hourly_pickups['lag_pickups_5day'] = hourly_pickups['pickup_counts'].shift(lag_hours_day)\n",
        "hourly_dropoff['lag_dropoffs_5day'] = hourly_dropoff['dropoff_counts'].shift(lag_hours_day)\n",
        "\n",
        "lag_hours_day= 144\n",
        "hourly_pickups['lag_pickups_6day'] = hourly_pickups['pickup_counts'].shift(lag_hours_day)\n",
        "hourly_dropoff['lag_dropoffs_6day'] = hourly_dropoff['dropoff_counts'].shift(lag_hours_day)\n",
        "\n",
        "lag_hours_day= 168\n",
        "hourly_pickups['lag_pickups_7day'] = hourly_pickups['pickup_counts'].shift(lag_hours_day)\n",
        "hourly_dropoff['lag_dropoffs_7day'] = hourly_dropoff['dropoff_counts'].shift(lag_hours_day)\n",
        "\n",
        "hourly_pickups=hourly_pickups.dropna()\n",
        "hourly_dropoff=hourly_dropoff.dropna()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aj-7eKmqYRNg"
      },
      "source": [
        "After hours of trying to  find the optimal lags to use in our model we conclude that makes sense to take as input for our train model the information from previous days. Towards this we compute the following lags: [24,48,72,96,120,144,168] which translates to 1st day, 2nd day , 3rd day... till day 7 which is the end of the week. In our first attempt we included lag 1 which means we take information for 1 hour ago which does not allign with what our task is. Therefore we believe the best lags to chose is the above approach."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5eCIiurYRNh",
        "outputId": "7f742ee4-1116-4c6b-95f6-4e943356f5b3"
      },
      "outputs": [],
      "source": [
        "# Extract day of the week, month, and hour\n",
        "hourly_pickups['day_of_week'] = hourly_pickups.index.dayofweek\n",
        "hourly_pickups['month'] = hourly_pickups.index.month\n",
        "hourly_pickups['hour'] = hourly_pickups.index.hour\n",
        "\n",
        "\n",
        "# Display the DataFrame\n",
        "print(hourly_pickups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exG5HA9TYRNh"
      },
      "source": [
        "Again in our first attempt we did not include the date features such as the month, week and day extracted from startime and stoptime accordingly and therefore as expected the Linear Regression model performed better, suggesting the strong linearity of our data.  In order to avoid overfitting and create a more accurate with better results in terms of r^2 we chose to include also the date features."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 822
        },
        "id": "LG_iAszSYRNi",
        "outputId": "a19fa603-806b-418c-d82b-425f29acd038"
      },
      "outputs": [],
      "source": [
        "# Extract day of the week, month, and hour\n",
        "hourly_dropoff['day_of_week'] = hourly_dropoff.index.dayofweek\n",
        "hourly_dropoff['month'] = hourly_dropoff.index.month\n",
        "hourly_dropoff['hour'] = hourly_dropoff.index.hour\n",
        "\n",
        "\n",
        "# Display the DataFrame\n",
        "hourly_dropoff"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Importantly, we followed the task's parameters by training our models on data from January to October and testing them on data from November and December"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yr1sPDjsYRNi",
        "outputId": "92835cea-7b08-4c21-f5e6-8b4dd3091a0b"
      },
      "outputs": [],
      "source": [
        "#Now we are ready to fit our models and make the predictions:\n",
        "def fit_and_evaluate(df, target_col, split_date):\n",
        "    split_date = pd.to_datetime(split_date)\n",
        "    train = df[df.index < split_date]\n",
        "    test = df[df.index >= split_date]\n",
        "\n",
        "    X_train = train.drop(target_col, axis=1)\n",
        "    y_train = train[target_col]\n",
        "    X_test = test.drop(target_col, axis=1)\n",
        "    y_test = test[target_col]\n",
        "\n",
        "    model = LinearRegression(fit_intercept=False)\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    return r2\n",
        "\n",
        "\n",
        "split_date = '2018-11-01'\n",
        "r2_pickups = fit_and_evaluate(hourly_pickups, 'pickup_counts', split_date)\n",
        "r2_dropoff = fit_and_evaluate(hourly_dropoff, 'dropoff_counts', split_date)\n",
        "\n",
        "print(f\"RÂ² Score for Pickups: {r2_pickups}\")\n",
        "print(f\"RÂ² Score for Dropoffs:{r2_dropoff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db0yscajYRNk"
      },
      "source": [
        "- **Linear Regression for Pickups:** For predicting pickups, the Linear Regression model had an R2 score of  0.6628, which indicates a linear relationship between the features and the pickup counts, with the model accounting for around 66% of the variance in the data. The model's performance is decent, showing that linear parameters such as time of day and historical data work as predictors of bike pickup demand.\n",
        "\n",
        "- **Linear Regression for Dropoffs:** Similar to pickups, the model scored 0.6616 for dropoffs, showing slightly lower performance in forecasting dropoff numbers than pickups. This means the model's linear relationships, which include time-related features and historical demand, accurately represent the dropoff patterns in bike-sharing activity."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh3-Pn2JYRNk"
      },
      "source": [
        "#### Random Forest Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T99TlurDYRNk",
        "outputId": "1325fa2e-3c41-4388-8003-a371d5faa704"
      },
      "outputs": [],
      "source": [
        "def fit_and_evaluate_with_predictions(df, target_col, split_date):\n",
        "    split_date = pd.to_datetime(split_date)\n",
        "    train = df[df.index < split_date]\n",
        "    test = df[df.index >= split_date]\n",
        "\n",
        "    X_train = train.drop(target_col, axis=1)\n",
        "    y_train = train[target_col]\n",
        "    X_test = test.drop(target_col, axis=1)\n",
        "    y_test = test[target_col]\n",
        "\n",
        "    model = RandomForestRegressor(n_estimators=256, max_depth=8, random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    r2 = r2_score(y_test, predictions)\n",
        "    return r2, y_test, predictions\n",
        "\n",
        "split_date = '2018-11-01'\n",
        "r2_pickups, y_test_pickups, predictions_pickups = fit_and_evaluate_with_predictions(hourly_pickups, 'pickup_counts', split_date)\n",
        "r2_dropoff, y_test_dropoff, predictions_dropoff = fit_and_evaluate_with_predictions(hourly_dropoff, 'dropoff_counts',split_date)\n",
        "\n",
        "print(f\"RÂ² Score for Pickups: {r2_pickups}\")\n",
        "print(f\"RÂ² Score for Dropoffs:Â {r2_dropoff}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eUotlMX0YRNo"
      },
      "source": [
        "- **Random Forest for Pickups:** The Random Forest model gave a bit higher R2 score of 0.6900 for pickups than the Linear Regression model, indicating a more robust prediction power, which makes sense as we decided to include the date features. \n",
        "\n",
        "- **Random Forest for Dropoffs:** The Random Forest model beats the Linear Regression model again for dropoffs with an R2 value of 0.6900. This illustrates the complexity of the dropoff demand patterns, which benefit from the Random Forest's deeper, non-linear modeling approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiKCRqqqYRNp"
      },
      "source": [
        "#### Predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "veie5wZ1YRNq",
        "outputId": "872a35ec-8762-48a8-af6f-6cd77bbd54ec"
      },
      "outputs": [],
      "source": [
        "#For RandomForest as is the model that performs best\n",
        "def plot_actual_vs_predicted_scatter(y_test, predictions, title):\n",
        "    fig = px.scatter(x=y_test, y=predictions, labels={'x': 'Actual', 'y': 'Predicted'}, title=title)\n",
        "    fig.add_shape(type='line', line=dict(color='black', width=2, dash='dash'), x0=y_test.min(), x1=y_test.max(), y0=y_test.min(), y1=y_test.max())\n",
        "    fig.update_layout(showlegend=False)\n",
        "    fig.show()\n",
        "\n",
        "plot_actual_vs_predicted_scatter(y_test_pickups, predictions_pickups, 'Actual vs Predicted - Pickups')\n",
        "plot_actual_vs_predicted_scatter(y_test_dropoff, predictions_dropoff, 'Actual vs Predicted - Dropoffs')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aL-RUS4dYRNq"
      },
      "source": [
        "- **Pickups plot:** From the actual vs predicted plot we can clearly say that there is a significant linear relationship, implying that the pickup model has good predictive accuracy. However, the density of points near the bottom show that the model is more accurate for smaller demand levels. As the actual values increase, there is some difference, indicating that the model may be less accurate at higher demand levels.\n",
        "\n",
        "- **Dropoffs plot:** Dropoff predictions follow a similar pattern to pickup predictions, with a good relationship between actual and predictedÂ values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYHAr6WrYRNr"
      },
      "source": [
        "#### Residual analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "FGmnx3OSYRNs",
        "outputId": "57b6085e-8ae5-4dbd-de0d-3f687497cd27"
      },
      "outputs": [],
      "source": [
        "mae_pickups = mean_absolute_error(y_test_pickups, predictions_pickups)\n",
        "mse_pickups = mean_squared_error(y_test_pickups, predictions_pickups)\n",
        "rmse_pickups = np.sqrt(mse_pickups)\n",
        "\n",
        "mae_dropoff = mean_absolute_error(y_test_dropoff, predictions_dropoff)\n",
        "mse_dropoff = mean_squared_error(y_test_dropoff, predictions_dropoff)\n",
        "rmse_dropoff = np.sqrt(mse_dropoff)\n",
        "\n",
        "print(f\"Pickups - MAE: {mae_pickups}, RMSE: {rmse_pickups}\")\n",
        "print(f\"Dropoffs - MAE: {mae_dropoff}, RMSE: {rmse_dropoff}\")\n",
        "\n",
        "residuals_pickups = y_test_pickups - predictions_pickups\n",
        "residuals_dropoff = y_test_dropoff - predictions_dropoff\n",
        "\n",
        "fig_res_pickups = px.scatter(x=predictions_pickups, y=residuals_pickups, labels={'x': 'Predictions', 'y': 'Residuals'}, title='Residuals vs Predictions for Pickups')\n",
        "fig_res_pickups.update_layout(shapes=[dict(type='line', y0=0, y1=0, x0=predictions_pickups.min(), x1=predictions_pickups.max(), line=dict(color='red', dash='dash'))])\n",
        "fig_res_pickups.show()\n",
        "\n",
        "fig_res_dropoff = px.scatter(x=predictions_dropoff, y=residuals_dropoff, labels={'x': 'Predictions', 'y': 'Residuals'}, title='Residuals vs Predictions for Dropoffs')\n",
        "fig_res_dropoff.update_layout(shapes=[dict(type='line', y0=0, y1=0, x0=predictions_dropoff.min(), x1=predictions_dropoff.max(), line=dict(color='red', dash='dash'))])\n",
        "fig_res_dropoff.show()\n",
        "\n",
        "fig_hist_pickups = px.histogram(x=residuals_pickups, nbins=50, opacity=0.7, labels={'x': 'Residuals'}, title='Histogram of Residuals for Pickups')\n",
        "fig_hist_pickups.show()\n",
        "\n",
        "fig_hist_dropoff = px.histogram(x=residuals_dropoff, nbins=50, opacity=0.7, labels={'x': 'Residuals'}, title='Histogram of Residuals for Dropoffs')\n",
        "fig_hist_dropoff.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2leaMOaYRNt"
      },
      "source": [
        "We see a continuous concentration of residuals around the zero line in the residual plots for our predictive models, which indicates greatÂ forecast accuracy for both pickups and dropoffs. Notably, the spread of residuals increases with prediction magnitude, implying that the model may have limits in capturing peak demand behaviors. The histograms show a right-skewed distribution of residuals, meaning that the model tends to under-predict rather than over-predict, particularly during periods of increasing demand. Our error metrics, MAE and RMSE, measure these differences further, with both indicating a reasonable but improveable model performance. The frequency of bigger mistakes, as shown by RMSE values greater than the MAE, indicates that there are larger errors, which areÂ areas of focus for model improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-72VTFY9YRNt"
      },
      "source": [
        "#### Demand Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "YxpgSstMYRNu",
        "outputId": "7df6d79c-96b6-4345-d0fe-5895147e054e"
      },
      "outputs": [],
      "source": [
        "pickups = predictions_pickups[:24]\n",
        "dropoffs = predictions_dropoff[:24]\n",
        "\n",
        "net_bikes_per_hour = [np.ceil(dropoffs[i] - pickups[i]) for i in range(len(pickups))]\n",
        "\n",
        "fig_net_bikes = go.Figure()\n",
        "fig_net_bikes.add_trace(go.Scatter(x=list(range(len(net_bikes_per_hour))), y=net_bikes_per_hour, mode='lines+markers', name='Net Bikes'))\n",
        "fig_net_bikes.update_layout(\n",
        "    title='Net Bikes per Hour',\n",
        "    xaxis_title='Hour of Day',\n",
        "    yaxis_title='Net Bikes (Dropoffs - Pickups)',\n",
        "    xaxis=dict(tickmode='array', tickvals=list(range(len(net_bikes_per_hour))), tickangle=90)  # Rotate x-axis labels vertically\n",
        ")\n",
        "fig_net_bikes.show()\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 559
        },
        "id": "w-vY3rgpYRNu",
        "outputId": "8ee2bd9f-b49c-4dbe-a135-368bed0ab8e6"
      },
      "outputs": [],
      "source": [
        "#cumulative net bikes\n",
        "cumulative_net_bikes = [sum(net_bikes_per_hour[:i+1]) for i in range(len(net_bikes_per_hour))]\n",
        "\n",
        "fig_cumulative_bikes = go.Figure()\n",
        "fig_cumulative_bikes.add_trace(go.Scatter(x=list(range(len(cumulative_net_bikes))), y=cumulative_net_bikes, mode='lines+markers', name='Cumulative Net Bikes', line=dict(color='red')))\n",
        "fig_cumulative_bikes.update_layout(title='Cumulative Net Bikes at Station', xaxis_title='Hour of Day', yaxis_title='Cumulative Net Bikes', xaxis=dict(tickmode='array', tickvals=list(range(len(cumulative_net_bikes)))))\n",
        "fig_cumulative_bikes.show()\n",
        "\n",
        "#maximum deficit of bikes (largest negative number)\n",
        "max_deficit = min(cumulative_net_bikes)\n",
        "bikes_required = math.ceil(abs(max_deficit))\n",
        "\n",
        "print(f'The number of bikes required at the start of the day to ensure no shortage is: {bikes_required}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzQ6JKPTYRNv"
      },
      "source": [
        "We used a method based on data to compute the optimal number of bikes that should be available at each station cluster in advance of the next day's demand. What we did is determined the net bikes per hour by taking the projected number of pickups and dropoffs for the first 24 hours of the next day and subtracting the difference between arrivals (dropoffs) and departures (pickups). This step gave us an hourly net flow of bikes at the stations.\n",
        "The net bikes per hour plot depicts the changing dynamics of bike consumption, telling us as when and where bikes may be in surplus or deficit. We averaged these net values to determine the total net bikes at each station to guarantee that there is never a lack of bikes during the day. The cumulative net bike graph indicated the important times in time when the bike deficit was at its greatest.\n",
        "We calculated the minimum number of bikes that must be moved to stations overnight by finding the biggest negative value in the cumulative net bikes, which shows the largest deficit. The results of this evaluation, which was based on the realistic goal of reducing any potential bike limited availability, led to the need for 154 bikes to be added to the network at the start of the day."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg7jd0kqYRNw"
      },
      "source": [
        "### Section 3 : Exploratory Component"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HaMl6SZ1YRNw"
      },
      "source": [
        "In the 3rd Section of our porject we examined how much the weather influence the demand of the bikes of New York. The data  contain infromation for the weather of the  year 2018 and they were taken from  https://www.visualcrossing.com/. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_UJHpVlhYRNx",
        "outputId": "1e3db27d-87c2-435c-b169-77bfd1fcaf71"
      },
      "outputs": [],
      "source": [
        "with open('NYC2018.csv', 'r') as f:\n",
        "    w = pd.read_csv(f)\n",
        "\n",
        "w.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY9Xm0gQYRNy"
      },
      "source": [
        "First step as always is explore and preprocesse our data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x5TUzQIiYRNy"
      },
      "outputs": [],
      "source": [
        "# Dropping irrelevant columns\n",
        "columns_to_remove = ['preciptype', 'windgust', 'severerisk']\n",
        "\n",
        "w = w.drop(columns=columns_to_remove)\n",
        "w.dropna(inplace=True)\n",
        "w.drop_duplicates(inplace=True)\n",
        "\n",
        "# Encoding date and time\n",
        "w['datetime'] = pd.to_datetime(w['datetime'])\n",
        "w['date'] = w['datetime'].dt.date\n",
        "w['hour'] = w['datetime'].dt.hour\n",
        "w['day_of_week'] = w['datetime'].dt.dayofweek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEccEAm0YRNz",
        "outputId": "3edd833c-0a6e-4954-b71c-5b8c069fb28c"
      },
      "outputs": [],
      "source": [
        "# Merge the weather data with the pickups and dropoffs dataframes.\n",
        "w_pickups = pd.merge(w, hourly_pickups, left_on='datetime', right_index=True, how='left')\n",
        "w_dropoffs = pd.merge(w, hourly_dropoff, left_on='datetime', right_index=True, how='left')\n",
        "print(w_pickups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "99tNz5XqYRN0"
      },
      "source": [
        "The weather data and the corresponding hourly transportation data were merged based on the 'datetime' column in the weather dataframe and the transportation dataframes' index."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x3F_Ygxbiuvq"
      },
      "outputs": [],
      "source": [
        "# Fill pickup counts with the mean of the previous and next hour\n",
        "w_pickups['pickup_counts'] = w_pickups['pickup_counts'].fillna((w_pickups['pickup_counts'].shift() + w_pickups['pickup_counts'].shift(-1)) / 2)\n",
        "\n",
        "w_pickups['lag_pickups_24hr'] = w_pickups['lag_pickups_24hr'].fillna((w_pickups['lag_pickups_24hr'].shift() + w_pickups['lag_pickups_24hr'].shift(-1)) / 2)\n",
        "w_pickups['lag_pickups_2day'] = w_pickups['lag_pickups_2day'].fillna((w_pickups['lag_pickups_2day'].shift() + w_pickups['lag_pickups_2day'].shift(-1)) / 2)\n",
        "w_pickups['lag_pickups_3day'] = w_pickups['lag_pickups_3day'].fillna((w_pickups['lag_pickups_3day'].shift() + w_pickups['lag_pickups_3day'].shift(-1)) / 2)\n",
        "w_pickups['lag_pickups_4day'] = w_pickups['lag_pickups_4day'].fillna((w_pickups['lag_pickups_4day'].shift() + w_pickups['lag_pickups_4day'].shift(-1)) / 2)\n",
        "w_pickups['lag_pickups_5day'] = w_pickups['lag_pickups_5day'].fillna((w_pickups['lag_pickups_5day'].shift() + w_pickups['lag_pickups_5day'].shift(-1)) / 2)\n",
        "w_pickups['lag_pickups_6day'] = w_pickups['lag_pickups_6day'].fillna((w_pickups['lag_pickups_6day'].shift() + w_pickups['lag_pickups_6day'].shift(-1)) / 2)\n",
        "w_pickups['lag_pickups_7day'] = w_pickups['lag_pickups_7day'].fillna((w_pickups['lag_pickups_7day'].shift() + w_pickups['lag_pickups_7day'].shift(-1)) / 2)\n",
        "\n",
        "\n",
        "# Fill dropoff counts with the mean of the previous and next hour\n",
        "w_dropoffs['dropoff_counts'] = w_dropoffs['dropoff_counts'].fillna((w_dropoffs['dropoff_counts'].shift() + w_dropoffs['dropoff_counts'].shift(-1)) / 2)\n",
        "\n",
        "w_dropoffs['lag_dropoffs_24hr'] = w_dropoffs['lag_dropoffs_24hr'].fillna((w_dropoffs['lag_dropoffs_24hr'].shift() + w_dropoffs['lag_dropoffs_24hr'].shift(-1)) / 2)\n",
        "w_dropoffs['lag_dropoffs_2day'] = w_dropoffs['lag_dropoffs_2day'].fillna((w_dropoffs['lag_dropoffs_2day'].shift() + w_dropoffs['lag_dropoffs_2day'].shift(-1)) / 2)\n",
        "w_dropoffs['lag_dropoffs_3day'] = w_dropoffs['lag_dropoffs_3day'].fillna((w_dropoffs['lag_dropoffs_3day'].shift() + w_dropoffs['lag_dropoffs_3day'].shift(-1)) / 2)\n",
        "w_dropoffs['lag_dropoffs_4day'] = w_dropoffs['lag_dropoffs_4day'].fillna((w_dropoffs['lag_dropoffs_4day'].shift() + w_dropoffs['lag_dropoffs_4day'].shift(-1)) / 2)\n",
        "w_dropoffs['lag_dropoffs_5day'] = w_dropoffs['lag_dropoffs_5day'].fillna((w_dropoffs['lag_dropoffs_5day'].shift() + w_dropoffs['lag_dropoffs_5day'].shift(-1)) / 2)\n",
        "w_dropoffs['lag_dropoffs_6day'] = w_dropoffs['lag_dropoffs_6day'].fillna((w_dropoffs['lag_dropoffs_6day'].shift() + w_dropoffs['lag_dropoffs_6day'].shift(-1)) / 2)\n",
        "w_dropoffs['lag_dropoffs_7day'] = w_dropoffs['lag_dropoffs_7day'].fillna((w_dropoffs['lag_dropoffs_7day'].shift() + w_dropoffs['lag_dropoffs_7day'].shift(-1)) / 2)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "JEiXggVRYRN2",
        "outputId": "3cb9bb33-ea1c-4c95-f3de-1b4d677f8ef1"
      },
      "outputs": [],
      "source": [
        "w_pickups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "5mffc1g0YRN2",
        "outputId": "4190f130-fd74-40cc-a628-5a767ef42b9b"
      },
      "outputs": [],
      "source": [
        "w_dropoffs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AXb6bPK-YRN3"
      },
      "source": [
        "We used the mean of the nearby hours to fill in the gaps in the pickup and dropoff counts, as well as their lag features, in order to handle missing values in our combined datasets. For accurate analysis, this method aids in preserving the continuity and integrity of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MmEgDbnRYRN3",
        "outputId": "03f1b426-0dba-4dc2-a3c1-6300ce093ead"
      },
      "outputs": [],
      "source": [
        "w_dropoffs.isnull().sum()\n",
        "w_pickups.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TcLvOZEYYRN4",
        "outputId": "0b5bcd6c-7fde-4196-db89-6d0d906b8878"
      },
      "outputs": [],
      "source": [
        "# Exclude non-numeric columns from mean calculation\n",
        "numeric_columns = w_dropoffs.select_dtypes(include=['float64', 'int64']).columns\n",
        "w_dropoffs[numeric_columns] = w_dropoffs[numeric_columns].fillna(w_dropoffs[numeric_columns].mean())\n",
        "\n",
        "numeric_columns = w_pickups.select_dtypes(include=['float64', 'int64']).columns\n",
        "w_pickups[numeric_columns] = w_pickups[numeric_columns].fillna(w_pickups[numeric_columns].mean())\n",
        "\n",
        "print(w_dropoffs.isnull().sum())\n",
        "print(w_pickups.isnull().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ssVwwMIGYRN5",
        "outputId": "5c992f40-68c0-4410-841f-3a899430769b"
      },
      "outputs": [],
      "source": [
        "pickup_stats = w_pickups['pickup_counts'].describe()\n",
        "weather_stats = w_pickups[['temp', 'feelslike', 'dew', 'humidity']].describe()\n",
        "\n",
        "correlation_matrix = w_pickups[['pickup_counts', 'temp', 'feelslike', 'dew', 'humidity']].corr()\n",
        "\n",
        "print(\"Pickup Counts Statistics:\")\n",
        "print(pickup_stats)\n",
        "print(\"\\nWeather Statistics:\")\n",
        "print(weather_stats)\n",
        "print(\"\\nCorrelation Matrix:\")\n",
        "print(correlation_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6qX-sJ6YRN6"
      },
      "source": [
        "For pickup counts and weather conditions, we performed correlation analysis and descriptive statistics. Significant variability was evident in the pickup counts, with an average of 51.88 and a standard deviation of 45.14. The average temperature and humidity, according to weather statistics, were 13.74Â°C and 67.65%, respectively. The temperature (both actual and feels like) and pickup counts showed a moderately positive correlation in our correlation matrix, indicating that warmer weather may result in more pickups. It's interesting to note that there was a negative correlation found between pickup counts and humidity, suggesting that higher humidity could lower pickup frequency. This kind of analysis offers insightful information about how weather might affect transportation patterns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        },
        "id": "hmkHcIThYRN7",
        "outputId": "f84a2652-1005-4dca-ee60-eb0eb541b121"
      },
      "outputs": [],
      "source": [
        "# Weather Condition Analysis\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.boxplot(x='conditions', y='pickup_counts', data=w_pickups)\n",
        "plt.title('Pickup Counts by Weather Conditions')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EIiNcfgYRN7"
      },
      "source": [
        "The boxplot illustrates how different weather conditions impact the frequency of pickups. When comparing to bad weather conditions like snow or rain, clear weather displays a greater range of pickup counts with a higher median. It's interesting to note that there are less pickups when there is rain and overcast conditions, as well as when there is snow and overcast conditions combined. This could mean that people are reluctant to travel in these situations. The number of pickups seems to be largely unaffected by partly cloudy weather, and a comparatively stable median indicates steady travel patterns. The correlation results are corroborated by this visual analysis, which shows that weather plays a major role in transportation patterns, with clear skies being the most conducive to higher pickup counts.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "6lhM0qKXYRN8",
        "outputId": "2b26ba43-a335-4368-81c1-fcade473c0dd"
      },
      "outputs": [],
      "source": [
        "correlation_data = w_pickups[['pickup_counts', 'temp', 'feelslike', 'dew', 'humidity']]\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CvAUtoirYRN9"
      },
      "source": [
        "Warmer weather may result in more pickups, as shown by the correlation heatmap in our report, which shows that temperature and \"feels like\" are somewhat positively correlated with pickup counts. On the other hand, humidity has a minor detrimental effect on pickups, which means that people might decide not to use transportation services when it's more humid. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvAUtoirYRN9"
      },
      "outputs": [],
      "source": [
        "correlation_data = w_pickups.drop(['datetime', 'name', 'conditions', 'icon', 'stations', 'date'], axis=1)\n",
        "correlation_matrix = correlation_data.corr()\n",
        "\n",
        "plt.figure(figsize=(15, 12))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
        "plt.title('Correlation Heatmap')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6JobKKHYRN-"
      },
      "source": [
        "#### Linear Regression Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "id": "cWux86wHj09q",
        "outputId": "314639ce-2b88-459a-b96f-5e87ef7b247c"
      },
      "outputs": [],
      "source": [
        "lag_features = ['lag_pickups_24hr', 'lag_pickups_2day', 'lag_pickups_7day']\n",
        "\n",
        "X = w_pickups[['temp'] + lag_features]\n",
        "y = w_pickups['pickup_counts']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model = LinearRegression()\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "r_squared_train = r2_score(y_train, y_train_pred)\n",
        "r_squared_test = r2_score(y_test, y_test_pred)\n",
        "print(f'Training R-squared: {r_squared_train}')\n",
        "print(f'Test R-squared: {r_squared_test}')\n",
        "\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.scatter(y_train, y_train_pred, label='Training Set', alpha=0.5)\n",
        "plt.scatter(y_test, y_test_pred, label='Test Set', alpha=0.5)\n",
        "plt.xlabel('Actual Pickup Counts')\n",
        "plt.ylabel('Predicted Pickup Counts')\n",
        "plt.title('Actual vs. Predicted Pickup Counts')\n",
        "plt.legend()\n",
        "\n",
        "y_combined = pd.concat([y_train, y_test])\n",
        "y_pred_combined = pd.concat([pd.Series(y_train_pred), pd.Series(y_test_pred)])\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.scatter(y_combined, y_pred_combined, s=0.75)\n",
        "plt.ylabel(\"Predicted Pickup Counts\")\n",
        "plt.xlabel(\"Actual Pickup Counts\")\n",
        "plt.plot([min(y_combined), max(y_combined)], [min(y_combined), max(y_combined)], color=\"red\", linestyle='--', linewidth=2)\n",
        "plt.title(\"Actual vs Predicted Pickup Counts\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eGYxwxspZTI",
        "outputId": "2c5bffad-7c07-4b39-b3f2-a5edb874be6d"
      },
      "outputs": [],
      "source": [
        "# Extract and display the coefficients\n",
        "feature_names = ['temp'] + lag_features\n",
        "coefficients = model.coef_\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    print(f'{feature}: {coef}')\n",
        "\n",
        "for feature, coef in zip(feature_names, coefficients):\n",
        "    if coef > 0:\n",
        "        print(f\"Higher {feature} leads to an increase in bike pickups.\")\n",
        "    else:\n",
        "        print(f\"Higher {feature} leads to a decrease in bike pickups.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3zL8n-23YROA"
      },
      "source": [
        "#### Random Forest Regression Model Fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 962
        },
        "id": "MZy01TVJYROA",
        "outputId": "b3d91e28-5666-435a-ea34-af54cb3b5862"
      },
      "outputs": [],
      "source": [
        "lag_features = ['lag_pickups_24hr', 'lag_pickups_2day', 'lag_pickups_7day']\n",
        "\n",
        "X = w_pickups[['temp'] + lag_features]\n",
        "y = w_pickups['pickup_counts']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "model = RandomForestRegressor(n_estimators=100, max_depth=10, min_samples_split=5, random_state=42)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_train_pred = model.predict(X_train_scaled)\n",
        "y_test_pred = model.predict(X_test_scaled)\n",
        "\n",
        "r_squared_train = r2_score(y_train, y_train_pred)\n",
        "r_squared_test = r2_score(y_test, y_test_pred)\n",
        "\n",
        "print(f'Training R-squared: {r_squared_train}')\n",
        "print(f'Test R-squared: {r_squared_test}')\n",
        "\n",
        "\n",
        "plt.scatter(y_train, y_train_pred, label='Training Set', alpha=0.5)\n",
        "plt.scatter(y_test, y_test_pred, label='Test Set', alpha=0.5)\n",
        "plt.xlabel('Actual Pickup Counts')\n",
        "plt.ylabel('Predicted Pickup Counts')\n",
        "plt.title('Actual vs. Predicted Pickup Counts')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "y_combined = pd.concat([y_train, y_test])\n",
        "y_pred_combined = pd.concat([pd.Series(y_train_pred), pd.Series(y_test_pred)])\n",
        "\n",
        "plt.scatter(y_combined, y_pred_combined, s=0.75)\n",
        "plt.ylabel(\"Predicted Pickup Counts\")\n",
        "plt.xlabel(\"Actual Pickup Counts\")\n",
        "plt.plot([min(y_combined), max(y_combined)], [min(y_combined), max(y_combined)], color=\"red\", linestyle='--', linewidth=2)\n",
        "plt.title(\"Actual vs Predicted Pickup Counts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqfeMUxbYROB"
      },
      "source": [
        "#### Lasso"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gxb7b77KkcnT",
        "outputId": "3206d6f4-6b30-4939-b274-852ce3826ab2"
      },
      "outputs": [],
      "source": [
        "w_pickups.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 944
        },
        "id": "uDyK5xYuYROB",
        "outputId": "37e4fab1-a67c-465b-fcea-7c2c5cede4c6"
      },
      "outputs": [],
      "source": [
        "lag_features = ['lag_pickups_24hr', 'lag_pickups_2day', 'lag_pickups_7day']\n",
        "\n",
        "w_pickups['temp_lag'] = w_pickups['temp'].shift(1)\n",
        "X = w_pickups[lag_features]\n",
        "y = w_pickups['pickup_counts']\n",
        "\n",
        "X.dropna(inplace=True)\n",
        "y = y[X.index]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
        "lasso = Lasso(random_state=42)\n",
        "lasso_grid = GridSearchCV(lasso, param_grid, cv=5, scoring='r2')\n",
        "lasso_grid.fit(X_train_scaled, y_train)\n",
        "best_alpha = lasso_grid.best_params_['alpha']\n",
        "lasso_model = Lasso(alpha=best_alpha, random_state=42)\n",
        "lasso_model.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = lasso_model.predict(X_test_scaled)\n",
        "\n",
        "r_squared = r2_score(y_test, y_pred)\n",
        "print(f'R-squared (Lasso): {r_squared}')\n",
        "\n",
        "\n",
        "plt.scatter(y_train, y_train_pred, label='Training Set', alpha=0.5)\n",
        "plt.scatter(y_test, y_test_pred, label='Test Set', alpha=0.5)\n",
        "plt.xlabel('Actual Pickup Counts')\n",
        "plt.ylabel('Predicted Pickup Counts')\n",
        "plt.title('Actual vs. Predicted Pickup Counts')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "y_combined = pd.concat([y_train, y_test])\n",
        "y_pred_combined = pd.concat([pd.Series(y_train_pred), pd.Series(y_test_pred)])\n",
        "\n",
        "plt.scatter(y_combined, y_pred_combined, s=0.75)\n",
        "plt.ylabel(\"Predicted Pickup Counts\")\n",
        "plt.xlabel(\"Actual Pickup Counts\")\n",
        "plt.plot([min(y_combined), max(y_combined)], [min(y_combined), max(y_combined)], color=\"red\", linestyle='--', linewidth=2)\n",
        "plt.title(\"Actual vs Predicted Pickup Counts\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZ_9zeWStyVy"
      },
      "source": [
        "Firstly, after running three different models, we discovered that RandomForestRegression is the best. Through additional model analysis, we were able to determine that there is a positive correlation between temperature and the demand for bike sharing, as indicated by the coefficient of 11.7453 for the 'temp' variable, which means higher temperatures are linked to an increase in bike pickups, probably as a result of better riding conditions.\n",
        "\n",
        "Furthermore, the 24-hour lag feature has a coefficient of 57.8205, indicating that the demand from the previous day influences the current day's bike-sharing usage a lot. The coefficients for the 2-day, 7-day, and 24-hour lags are all positive, which shows that previous pickup trends are strong predictors of the current demand.  Compared to the 24-hour lag, the 2-day lag feature shows a decreased but still good, influence on current demand (coefficient of 2.6392). The largest coefficient, 110.1331, appears in the 7-day lag feature, which suggests a strong weekly cycle in the demand for bike sharing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3RbQsCPYROE"
      },
      "source": [
        "### Section 4: Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AbMAGomGYROE"
      },
      "source": [
        "| Section         | Findings                                                                                                          | Further improvements                         |\n",
        "|-------------------------------|-----------------------------------------------------------------------------------------------------------------------------------|----------------------------------------------------------|\n",
        "| *Data Preproccesing*             | We cleaned both datasets NYC2018 and Trips2018 by handling missing data, performing filtering and visualizing the outcomes.                                    | Based on the given dataset, the preprocessing we performed is adequate. Perhaps more visualization could have provided more insights for further filtering.  |\n",
        "| *Model Selection*           | We performed Linear Regression and RandomForestRegression and the last one identified as slightly better  based on predictive accuracy and suitability for the data characteristics.  | By adding more features such as the season of the year, the morning/evening rush and checking whether the previous day was a weekend or a special event might have improved our model in terms of R2 accuracy.|\n",
        "| *EDA / Temperature's Impact*      | Positive correlation between temperature (coefficient: 11.7453) and bike-sharing demand suggests higher demand rates in better weather conditions. Thus, the answer to our EDA research question on whether weather affects rental behavior is affirmative, and the outcomes align with our expectations. Suggests the need for adaptive operational strategies during warmer periods to meet increased demand.          |  Further questions could be addressed in this area, such as whether inaccurate weather forecasts affect bike rentals. Additionally, data from nearby modes of transport could be utilized, and their correlation with riding activity can also be examined.\n",
        "| *Lagged Features Analysis*  | Strong influence of 24-hour, 2-day, and 7-day lag features on current demand, highlighting consistent daily and weekly patterns.  | Emphasizes the importance of considering recent usage history for accurate demand forecasting and planning. Additionally, data from previous years can be utilized, and features such as 'lags_1year' can be imported forÂ thisÂ purpose.|"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Contributon table"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "| Sections          | Team Member                                                                                           |                                            \n",
        "|----------------------|--------------------------------------------------------------------------------------------------------|\n",
        "|  Intoduction    | Elli Georgiou, Maria Katarachia,Stavroula Douva, Michail-Achillefs Katarachias, Dimitris Voukatas     |\n",
        "| PCA, Elbow method & Clustering   | Michail-Achillefs Katarachias  |\n",
        "| Prediction Model   | Elli Georgiou,  Michail-Achillefs Katarachias, Dimitris Voukatas   |\n",
        "| EDA    |Maria Katarachia, Stavroula Douva|\n",
        "| Conclusions    |     Elli Georgiou, Dimitris Voukatas  |\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "deep_learning_seg",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
