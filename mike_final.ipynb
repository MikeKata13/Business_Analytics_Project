{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Business Analytics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Participants: \n",
    "\n",
    "- Elli Georgiou: s223408\n",
    "- Maria Katarachia: s213633\n",
    "- Stavroula Douva: s222652\n",
    "- Michail-Achillefs Katarachias: s222653 \n",
    "- Dimitris Voukatas: s230148\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of Content\n",
    "Section 1: **Introduction + Data Analysis \\& Visualizations**<br> <br>\n",
    "Section 2: **Prediction models**<br> <br>\n",
    "Section 3: **Exploratory Component** <br> <br>\n",
    "Section 4: **Conclusions**<br> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 : Introduction and Data Analysis and Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this first section, we embark on the analysis of our dataframe, aiming to transform it into a higher-quality set that can provide meaningful insights for the subsequent phases of this project. The first crucial step involves preprocessing, which includes cleaning and organizing the dataset.\n",
    "\n",
    "Cleaning is of paramount importance, encompassing tasks such as handling missing values, removing duplicates, and addressing outliers. These actions ensure that the data is reliable and free from errors, setting the foundation for robust analysis.\n",
    "\n",
    "Following this approach, we filter the data based on relevant criteria. For instance, we seek data that adheres to logical parameters; for example, age should fall within a specific range, trip duration should follow a consistent format, and station locations must be within the boundaries of New York territory.\n",
    "\n",
    "Once the data is cleaned and preprocessed, visualization emerges as a powerful tool for gaining initial insights. Visual representations, such as histograms, scatter plots, and correlation matrices, offer a holistic view, unveiling potential patterns, trends, or relationships throughout the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly, we import all the libraries that will be used in this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import plotly.express as px\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data importing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We open the file and create a folder for our plots as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'Trips_2018.csv'\n",
    "\n",
    "if not os.path.exists('plots'):\n",
    "    os.makedirs('plots')\n",
    "\n",
    "with open(file_path, 'r') as f:\n",
    "    df = pd.read_csv(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at how our dataframe appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are checking the data types of our dataframe to see that are on the appropiate type and also the dimensions of our dataframe.\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are dropping the first column because it is just an index.\n",
    "df = df.drop(df.columns[0], axis=1) \n",
    "df = df.sample(n=5000000, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We check if there are any null values in our dataframe.\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We also check that there are no duplicates.\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will remove the rows from the dataset that have null or non-numeric values in \"start_station_id\" and \"end_station_id.\" The dataset only contains rows with valid numeric station IDs by converting these columns to numeric using the 'coerce' option. Any non-numeric values are then set as NaN, and rows containing these NaN values will be removed. The dataset's integrity will be improved for upcoming analyses with this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We remove null values for start_station_id and end_station_id.\n",
    "df = df[pd.to_numeric(df['start_station_id'], errors='coerce').notnull()]\n",
    "df = df[pd.to_numeric(df['end_station_id'], errors='coerce').notnull()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " A great deal of focus was put on standardizing and extracting important time-based components from our dataset during the initial phase of our data analysis. We used a function to convert the columns to datetime format. This conversion guarantees accuracy and consistency in the time data representation, which is essential for any analysis that is time-sensitive. The date, hour, and day of the week, three crucial temporal components, were then taken out of the'start_time' column. This extraction plays a crucial role in providing a detailed analysis of the data, enabling insights into weekly and daily trends, and comprehending patterns of activity throughout the day. This fundamental stage of data processing paves the way for a more in-depth and perceptive examination, enabling us to identify significant trends and conclusionsfrom our dataset's temporal features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert start_time and end_time columns to datetime format.\n",
    "df['starttime'] = pd.to_datetime(df['starttime'])\n",
    "df['stoptime'] = pd.to_datetime(df['stoptime'])\n",
    "\n",
    "# Extract date, hour, and day of the week from start_time column.\n",
    "df['date'] = df['starttime'].dt.date\n",
    "df['hour'] = df['starttime'].dt.hour\n",
    "df['day_of_week'] = df['starttime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used strict filters during the data cleansing process to improve the quality of the data. The dataset was further refined by eliminating trips that fell outside of the 99th percentile of trip durations, as well as entries with negative or abnormally long trip durations (more than 24 hours). In order to prevent skewing age-related analysis, we also filtered out records whose birth years fell below the lower 99th percentile, keeping only entries with valid gender values (0, 1, or 2). These actions were essential to maintaining the integrity of the dataset and providing a strong basis for precise and perceptive analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop any rows with negative trip durations.\n",
    "df = df[df['tripduration'] > 0]\n",
    "\n",
    "# Drop any rows with trip durations greater than 24 hours.\n",
    "df = df[df['tripduration'] <= 3600]\n",
    "\n",
    "# Drop rows that are outside the 99th percentile of trip durations.\n",
    "df = df[df['tripduration'] <= df['tripduration'].quantile(.99)]\n",
    "\n",
    "# Drop rows where the birth_year is outside the lower 99th percentile of birth years.\n",
    "df = df[df['birth_year'] >= df['birth_year'].quantile(.01)]\n",
    "\n",
    "# Drop rows whsere the gender is not 0, 1 or 2.\n",
    "df = df[df['gender'].isin([0, 1, 2])]\n",
    "# Drop rows where the gender is not 0, 1 or 2.\n",
    "df = df[df['gender'].isin([0, 1, 2])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descriptive statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We visualize our data to gain useful insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution of trip durations\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(df['tripduration']/3600, bins=100)\n",
    "plt.xlabel('Trip Duration (hours)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Trip Durations')\n",
    "plt.savefig(os.path.join('plots', 'trip_duration_distribution.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The provided histogram displays the distribution of trip durations, expressed in hours, for a dataset of travels. The distribution seems to be right-skewed, suggesting that shorter trips are significantly more frequent than longer ones. Most trips are shorter than 0.1 hours, and as trip length increases, frequency gradually decreases. Very short trips appear to be the norm in this dataset, as indicated by a sharp decline beyond the 0.1-hour mark. Although these instances are comparatively uncommon, the existence of trips lasting up to 0.8 hours (nearly 50 minutes) suggests some variability in trip length. This graphic aids in comprehending the usual usage patterns and can be extremely important for operational planning, including staffing needs and inventory control. The skewness of the plot towards shorter durations may also have an impact on policy choices, such as price plans or special offers, to accommodate the most typical travel times."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are checking the age of our users, and it seems that our customers are within a reasonable age range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Age distribution of the users.\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.hist(2019 - df['birth_year'], bins=100)\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of User Ages')\n",
    "plt.savefig(os.path.join('plots', 'age_distribution.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The age distribution of users from the dataset is shown in the histogram. The age is determined by subtracting the user's birth year from the current year (2019). The distribution exhibits a bell-shaped curve with a prominent peak at thirty years of age, indicating that most users are between the ages of twenty and thirty. This peak indicates that there is a significant user concentration in this age range. Notably, there is an odd peak around the age of 50, w_pickupsich might be the result of a default input value, a data error, or a sizable user base of users in that age range. For younger (<20 years) and older (>60 years) age groups, the distribution tapers off, suggesting lower service engagement in these groups.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to observe the gender distrubution as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_counts = df['gender'].value_counts()\n",
    "\n",
    "gender_counts\n",
    "\n",
    "labels = ['Male', 'Female', 'Unknown']\n",
    "colors = ['blue', 'pink', 'gray']\n",
    "\n",
    "fig = plt.figure(facecolor='white')\n",
    "plt.pie(gender_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that the pie is drawn as a circle.\n",
    "plt.title('Gender Distribution')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gender distribution within a user dataset is shown visually in a pie chart. With 68.2% of the population, men make up the majority, followed by women with 23.2%. In addition, 8.6% of the user base is shown on the chart as \"Unknown,\" which may indicate users who would rather withhold their gender or other information that was not collected. The service may be more well-liked by men, as suggested by the preponderance of male users, or it may be a reflection of larger industry trends. The substantial percentage of users who do not identify their gender points to a potential improvement area for data collection, or it suggests a varied user base with a range of gender identities. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have completed the initial phase of our data preparation, we have a carefully selected dataframe that is ready for more complex analytical tasks. Our dataset is in the exact format needed for the complex tasks that lie ahead due to the transformation and cleaning processes that have been carried out. We have created a solid basis for our analysis by standardizing time-related columns to datetime formats, judiciously removing outliers, and carefully classifying demographic data like age and gender.\n",
    "\n",
    "\n",
    "With this improved dataset, we can now explore more complex clustering algorithms, which will help us identify underlying patterns and groupings in the data. Furthermore, the foundation we've laid enables us to confidently use predictive modeling methods. These models will make better predictions about future trends, user behavior, and service demands by utilizing the rich, cleaned data. Thus, in later phases of our project, our preparatory work will play a pivotal role in facilitating deeper insights and data-driven decision-making.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Descriptive study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have started the clustering phase of our ongoing data analysis evolution by using Principal Component Analysis (PCA) to comprehend the feature correlations in our cleaned dataset. The foundation for a successful PCA is laid by calculating the trip duration from the \"start_time\" and \"stop_time,\" and then normalizing the dataset to include station locations, birth years, and trip durations. In order to guarantee that every feature contributes proportionately to the analysis a normalization is necessary. The dataset is then transformed by the PCA procedure, w_pickupsich reduces it to its essential elements so that the maximum variance is captured without sacrificing the original complexity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tripduration'] = (df['stoptime'] - df['starttime']).dt.total_seconds()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data\n",
    "scaler = StandardScaler()\n",
    "scaled_df = scaler.fit_transform(df[['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'birth_year', 'tripduration']])\n",
    "scaled_df = pd.DataFrame(scaled_df, columns=['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'birth_year', 'tripduration'])\n",
    "\n",
    "# PCA using all the features\n",
    "pca = PCA(n_components=6)\n",
    "pca.fit(scaled_df)\n",
    "pca_df = pca.transform(scaled_df)\n",
    "pca_df = pd.DataFrame(pca.components_, columns=['start_station_latitude', 'start_station_longitude', 'end_station_latitude', 'end_station_longitude', 'birth_year', 'tripduration'])\n",
    "\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance')\n",
    "plt.savefig(os.path.join('plots', 'pca_cumulative_explained_variance.png'))\n",
    "plt.show()\n",
    "\n",
    "corr_matrix = scaled_df.corr()\n",
    "corr_matrix['tripduration'].sort_values(ascending=False)\n",
    "\n",
    "# Print the loadings for each principal component\n",
    "for i in range(len(pca_df)):\n",
    "    print(f\"\\nPrincipal Component {i + 1} Loadings:\")\n",
    "    print(pca_df.iloc[i].sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the start_station_latitude , end_station_longitude  contain plethora of information for our problem.       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the cumulative explained variance against the number of components helps us visualize this transformation and informs our choice of how many components to keep. Our Principal Component Analysis (PCA) shows that the first three components, with the first being particularly dominant, account for the majority of the variance, as shown by the cumulative explained variance plot. Upon examining the loadings, we find that the first two components are significantly influenced by geographic coordinates, suggesting that location plays a major role in the variance of our data. The third component is mainly affected by the length of the trip, which may indicate that the duration of use is not affected by geographic factors. This information is important because it shows that trip duration and spatial features, for our dataset, capture the essential elements of user behavior. The first component emphasizes geographical data, while the third component points to temporal aspects. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elbow method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to identify the optimal number of clusters for the clustering algorithm we chose the elbow method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract coordinates from the sample\n",
    "coordinates_sample = df[['start_station_latitude', 'start_station_longitude']]\n",
    "\n",
    "#Standardize the coordinates\n",
    "scaler = StandardScaler()\n",
    "coordinates_standardized = scaler.fit_transform(coordinates_sample)\n",
    "# coordinates_sample = coordinates_standardized\n",
    "\n",
    "#Elbow method\n",
    "distortions = []\n",
    "K = range(1, 40)  \n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init='auto')\n",
    "    kmeans.fit(coordinates_standardized)\n",
    "    distortions.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow curve\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(K, distortions, 'bx-')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Distortion (Inertia)')\n",
    "plt.title('Elbow Method for Optimal k with Sampled Data')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine the ideal number of clusters for K-means clustering, utilize the Elbow Method plot. It displays a decrease in inertia as the number of clusters increases by plotting the number of clusters against the within-cluster sum of squares. The rate of decrease changes sharply at the optimal point, or \"elbow,\" w_pickupsich in this case is approximately 5 clusters. This shows that after five clusters, more clusters do not significantly add to the variance explanation, suggesting that our data can be effectively divided into five groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further processing with K-means clustering to remove locations we don't care about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Finding the optimal number of clusters with elbow method\n",
    "\n",
    "# Extract coordinates from the sample\n",
    "coordinates_start = df[['start_station_latitude', 'start_station_longitude']]\n",
    "coordinates_end = df[['end_station_latitude', 'end_station_longitude']]\n",
    "\n",
    "#Standardize the coordinates\n",
    "scaler = StandardScaler()\n",
    "coordinates_standardized = scaler.fit_transform(coordinates_start)\n",
    "\n",
    "# Plot K-means with 20 clusters\n",
    "kmeans = KMeans(n_clusters=20, random_state=42)\n",
    "kmeans.fit(coordinates_standardized)\n",
    "df['cluster'] = kmeans.predict(coordinates_standardized)\n",
    "\n",
    "# Cluster the data into 20 clusters\n",
    "kmeans = KMeans(n_clusters=20, init='k-means++', random_state=42).fit(coordinates_start)\n",
    "\n",
    "# Get the cluster centers\n",
    "centers = kmeans.cluster_centers_\n",
    "\n",
    "# Add the cluster labels to the dataframe\n",
    "df['cluster'] = kmeans.predict(coordinates_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our standardized start station coordinates and the K-means algorithm, we were able to cluster our dataset into 20 different groups. Each data point was assigned to a cluster by this process, and the output showed the hubs of activity in the center. The popularity of the station and user trends can be inferred from this clustering, which can help with operational and strategic decisions for station development and management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the clusters on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "def plot_stations_map(stations):\n",
    "    #First before plotting we have to deal with the outliers \n",
    "    #The latitude of New York City is approximately between 40.4774 and 45.01585, and the longitude is approximately between -79.76259 and -71.18507.\n",
    "\n",
    "    lon_min = -79.76259\n",
    "    lat_min = 40.4774\n",
    "    lon_max = -71.18507\n",
    "    lat_max = 45.01585\n",
    "\n",
    "    # Store the stations that are within the boundaries\n",
    "    stations = stations[\n",
    "        (stations['start_station_latitude'] > lat_min) &\n",
    "        (stations['start_station_latitude'] < lat_max) &\n",
    "        (stations['start_station_longitude'] > lon_min) &\n",
    "        (stations['start_station_longitude'] < lon_max)\n",
    "    ]\n",
    "    \n",
    "    #Plot the stations with an underlying map of New York City\n",
    "    title = 'Citi Bike Stations in New York City'\n",
    "    fig = px.scatter_mapbox(\n",
    "        stations,\n",
    "        lat='start_station_latitude',\n",
    "        lon='start_station_longitude',\n",
    "        color='cluster',\n",
    "        mapbox_style='carto-positron',\n",
    "        zoom=9,\n",
    "        width=1000,\n",
    "        height=600\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title=dict(\n",
    "            text=title,\n",
    "            x=0.5,  # Center the title horizontally\n",
    "            xanchor='center',  # Anchor point for horizontal alignment\n",
    "            font=dict(size=20)\n",
    "        )\n",
    "    )\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the locations that are more than 3 standard deviations from the center of the clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the distance between each point and its cluster center\n",
    "distance = kmeans.transform(coordinates_start)\n",
    "\n",
    "# Get the minimum distance for each point and its cluster index\n",
    "min_distance = np.min(distance, axis=1)\n",
    "min_distance_cluster = np.argmin(distance, axis=1)\n",
    "\n",
    "threshold = 2*np.std(distance,axis=1)\n",
    "\n",
    "# Get the indices of the points that are within the threshold distance of a cluster center\n",
    "within_threshold = np.argwhere(min_distance < threshold).flatten()\n",
    "\n",
    "# Remove the points that are outside the threshold distance of a cluster center\n",
    "df = df.iloc[within_threshold]\n",
    "\n",
    "# Plot the stations with an underlying map of New York City.\n",
    "# plot_stations_map(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The stations were clustered to find patterns in their locations using the KMeans algorithm. Subsequently, the analysis concentrated on ensuring a tighter and more relevant grouping by eliminating outliers, or stations located more than two standard deviations from the cluster centers, in order to refine these clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final graphic shows a map with stations colored by the clusters to which they belong. This map shows the locations of bike stations throughout the city as well as areas with a high density of stations. These kinds of insights are crucial for operational optimization because they can direct the placement of new stations strategically and the distribution of resources in a way that best serves user demand. By using this data, the Citi Bike program will be able to improve its service offerings and gain a better understanding of how people use it in New York City's urban environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the most important cluster and get all the relevant values from the dataframe based on it (MVC - Most Valuable Cluster)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the clusters by the number of trips (instances) in each cluster \n",
    "sorted_clusters= df['cluster'].value_counts().idxmax()\n",
    "\n",
    "# Get the indices of the points that are in the most important cluster\n",
    "most_important_cluster_indices = np.argwhere(df['cluster'] == sorted_clusters).flatten()\n",
    "\n",
    "# Get the most important cluster's features\n",
    "MVC = df.iloc[most_important_cluster_indices].copy()\n",
    "MVC.info()\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preproccesing we keep only theccolumns we need\n",
    "columns_to_remove = ['start_station_id', 'start_station_latitude', 'start_station_longitude', 'end_station_id', 'end_station_latitude','end_station_longitude','bikeid','day_of_week','usertype','birth_year','gender']\n",
    "prediction_cluster = MVC.drop(columns=columns_to_remove, axis=1)\n",
    "prediction_cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pickups and dropoffs dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We order the elements of this dataframe based on the date.\n",
    "\n",
    "prediction_cluster.sort_values(by = ['starttime'],inplace = True )\n",
    "# We work on hourly intervals for pick ups\n",
    "pickups = prediction_cluster.copy()\n",
    "pickups.set_index('starttime', inplace=True)\n",
    "hourly_pickups = pickups.resample('H').count()  \n",
    "columns_to_drop = ['tripduration', 'stoptime', 'date', 'cluster']\n",
    "hourly_pickups = hourly_pickups.rename(columns={'hour': 'pickup_counts'})\n",
    "hourly_pickups = hourly_pickups.drop(columns=columns_to_drop)\n",
    "\n",
    "hourly_pickups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dropoffs = prediction_cluster.copy()\n",
    "dropoffs.set_index('stoptime', inplace=True)\n",
    "hourly_dropoffs = dropoffs.resample('H').count()\n",
    "columns_to_drop = ['tripduration', 'starttime', 'date', 'cluster']\n",
    "hourly_dropoffs = hourly_dropoffs.rename(columns={'hour': 'dropoff_counts'})\n",
    "hourly_dropoffs = hourly_dropoffs.drop(columns=columns_to_drop)\n",
    "hourly_dropoffs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming hourly_pickups and hourly_dropoffs are your DataFrames\n",
    "\n",
    "# Create lagged features\n",
    "def create_lagged_features(df, lag_hours_list):\n",
    "    for lag_hours in lag_hours_list:\n",
    "        lag_name = f'lag_{lag_hours}hr'\n",
    "        df[lag_name] = df.iloc[:, 0].shift(lag_hours)  # Assuming the count is the first column\n",
    "    return df.dropna()\n",
    "\n",
    "# Apply lagged features\n",
    "lag_hours_list = [1, 24, 168]  # 1 hour, 1 day, 1 week\n",
    "# lag_hours_list = [1,]  # only for the day before\n",
    "\n",
    "hourly_pickups = create_lagged_features(hourly_pickups, lag_hours_list)\n",
    "hourly_dropoffs = create_lagged_features(hourly_dropoffs, lag_hours_list)\n",
    "\n",
    "# Define a function to fit and evaluate the model\n",
    "def fit_and_evaluate(df, target_col, split_date):\n",
    "    split_date = pd.to_datetime(split_date)\n",
    "    train = df[df.index < split_date]\n",
    "    test = df[df.index >= split_date]\n",
    "\n",
    "    X_train = train.drop(target_col, axis=1)\n",
    "    y_train = train[target_col]\n",
    "    X_test = test.drop(target_col, axis=1)\n",
    "    y_test = test[target_col]\n",
    "\n",
    "    model = LinearRegression(fit_intercept=False)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    return r2\n",
    "\n",
    "# Evaluate models\n",
    "split_date = '2018-11-01'  # Adjust this date as per your dataset\n",
    "r2_pickups = fit_and_evaluate(hourly_pickups, 'pickup_counts', split_date)\n",
    "r2_dropoff = fit_and_evaluate(hourly_dropoffs, 'dropoff_counts', split_date)\n",
    "\n",
    "print(f\"R² Score for Pickups: {r2_pickups}\")\n",
    "print(f\"R² Score for Dropoffs:{r2_dropoff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a Random Forest Model to create predictions for the pickups and dropoffs of the stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "from datetime import datetime\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Assuming hourly_pickups and hourly_dropoffs are your DataFrames\n",
    "\n",
    "# Create lagged features\n",
    "def create_lagged_features(df, lag_hours_list):\n",
    "    for lag_hours in lag_hours_list:\n",
    "        lag_name = f'lag_{lag_hours}hr'\n",
    "        df[lag_name] = df.iloc[:, 0].shift(lag_hours)  # Assuming the count is the first column\n",
    "    return df.dropna()\n",
    "\n",
    "# Apply lagged features\n",
    "lag_hours_list = [1, 24, 168]  # 1 hour, 1 day, 1 week\n",
    "hourly_pickups = create_lagged_features(hourly_pickups, lag_hours_list)\n",
    "hourly_dropoffs = create_lagged_features(hourly_dropoffs, lag_hours_list)\n",
    "\n",
    "# Define a function to fit and evaluate the model\n",
    "def fit_and_evaluate(df, target_col, split_date):\n",
    "    split_date = pd.to_datetime(split_date)\n",
    "    train = df[df.index < split_date]\n",
    "    test = df[df.index >= split_date]\n",
    "\n",
    "    X_train = train.drop(target_col, axis=1)\n",
    "    y_train = train[target_col]\n",
    "    X_test = test.drop(target_col, axis=1)\n",
    "    y_test = test[target_col]\n",
    "\n",
    "    model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "    return r2 \n",
    "\n",
    "# Evaluate models\n",
    "split_date = '2018-11-01'  # Adjust this date as per your dataset\n",
    "r2_pickups = fit_and_evaluate(hourly_pickups, 'pickup_counts', split_date)\n",
    "r2_dropoff = fit_and_evaluate(hourly_dropoffs, 'dropoff_counts', split_date)\n",
    "\n",
    "print(f\"R² Score for Pickups: {r2_pickups}\")\n",
    "print(f\"R² Score for Dropoffs: {r2_dropoff}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Opening the weather dataset for NYC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('NYC2018.csv', 'r') as f:\n",
    "    w = pd.read_csv(f)\n",
    "\n",
    "w.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and formatting weather dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping irrelevant columns\n",
    "columns_to_remove = ['preciptype', 'windgust', 'severerisk']\n",
    "\n",
    "# Dropping na and duplicates\n",
    "w = w.drop(columns=columns_to_remove)\n",
    "w.dropna(inplace=True)\n",
    "w.drop_duplicates(inplace=True)\n",
    "\n",
    "# Encoding date and time\n",
    "w['datetime'] = pd.to_datetime(w['datetime'])\n",
    "\n",
    "# Extract date, hour, and day of the week from start_time column.\n",
    "w['date'] = w['datetime'].dt.date\n",
    "w['hour'] = w['datetime'].dt.hour\n",
    "w['day_of_week'] = w['datetime'].dt.dayofweek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge the weather data with the pickups and dropoffs dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_pickups = pd.merge(w, hourly_pickups, left_on='datetime', right_index=True, how='left')\n",
    "w_dropoffs = pd.merge(w, hourly_dropoffs, left_on='datetime', right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the missing values with the mean of the column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill pickup counts with the mean of the previous and next hour\n",
    "w_pickups['pickup_counts'] = w_pickups['pickup_counts'].fillna((w_pickups['pickup_counts'].shift() + w_pickups['pickup_counts'].shift(-1)) / 2)\n",
    "w_pickups['lag_1hr'] = w_pickups['lag_1hr'].fillna((w_pickups['lag_1hr'].shift() + w_pickups['lag_1hr'].shift(-1)) / 2)\n",
    "w_pickups['lag_24hr'] = w_pickups['lag_24hr'].fillna((w_pickups['lag_24hr'].shift() + w_pickups['lag_24hr'].shift(-1)) / 2)\n",
    "w_pickups['lag_168hr'] = w_pickups['lag_168hr'].fillna((w_pickups['lag_168hr'].shift() + w_pickups['lag_168hr'].shift(-1)) / 2)\n",
    "\n",
    "# Fill dropoff counts with the mean of the previous and next hour\n",
    "w_dropoffs['dropoff_counts'] = w_dropoffs['dropoff_counts'].fillna((w_dropoffs['dropoff_counts'].shift() + w_dropoffs['dropoff_counts'].shift(-1)) / 2)\n",
    "w_dropoffs['lag_1hr'] = w_dropoffs['lag_1hr'].fillna((w_dropoffs['lag_1hr'].shift() + w_dropoffs['lag_1hr'].shift(-1)) / 2)\n",
    "w_dropoffs['lag_24hr'] = w_dropoffs['lag_24hr'].fillna((w_dropoffs['lag_24hr'].shift() + w_dropoffs['lag_24hr'].shift(-1)) / 2)\n",
    "w_dropoffs['lag_168hr'] = w_dropoffs['lag_168hr'].fillna((w_dropoffs['lag_168hr'].shift() + w_dropoffs['lag_168hr'].shift(-1)) / 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistics on the weather data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive Statistics\n",
    "pickup_stats = w_pickups['pickup_counts'].describe()\n",
    "weather_stats = w_pickups[['temp', 'feelslike', 'dew', 'humidity']].describe()\n",
    "\n",
    "# Correlation Analysis\n",
    "correlation_matrix = w_pickups[['pickup_counts', 'temp', 'feelslike', 'dew', 'humidity']].corr()\n",
    "\n",
    "# Display the results\n",
    "print(\"Pickup Counts Statistics:\")\n",
    "print(pickup_stats)\n",
    "\n",
    "print(\"\\nWeather Statistics:\")\n",
    "print(weather_stats)\n",
    "\n",
    "print(\"\\nCorrelation Matrix:\")\n",
    "print(correlation_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot some important insights. We will work with the pickups, but the same applies for the dropoffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# Weather Condition Analysis\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x='conditions', y='pickup_counts', data=w_pickups)\n",
    "plt.title('Pickup Counts by Weather Conditions')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select relevant columns for correlation analysis\n",
    "correlation_data = w_pickups[['pickup_counts', 'temp', 'feelslike', 'dew', 'humidity']]\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Select relevant columns for correlation analysis\n",
    "correlation_data = w_pickups.drop(['datetime', 'name', 'conditions', 'icon', 'stations', 'date'], axis=1)\n",
    "\n",
    "# Calculate the correlation matrix\n",
    "correlation_matrix = correlation_data.corr()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(15, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "\n",
    "# Print the columns to verify their existence\n",
    "print(\"Columns before dropping:\", w_pickups.columns)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "w_pickups = w_pickups.drop(['name', 'icon', 'stations', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Print the columns after dropping\n",
    "print(\"Columns after dropping:\", w_pickups.columns)\n",
    "\n",
    "# Drop rows with NaN values resulting from lag operations\n",
    "w_pickups = w_pickups.dropna()\n",
    "\n",
    "# Select specific weather-related features\n",
    "weather_features = w_pickups[['temp', 'feelslike', 'dew', 'humidity', 'precip', 'precipprob', 'snow', 'snowdepth']]\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = weather_features\n",
    "y = w_pickups['pickup_counts']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a linear regression model\n",
    "model = make_pipeline(StandardScaler(), LinearRegression())\n",
    "\n",
    "# Fit the model on the training data\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')\n",
    "\n",
    "# Optionally, you can print the coefficients of the model\n",
    "print('Coefficients:', model.named_steps['linearregression'].coef_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit and plot our linear regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Linear Regression model\n",
    "r_squared = model.score(X_test, y_test)\n",
    "print(f'R^2: {r_squared}')\n",
    "\n",
    "y_pred_all = model.predict(X)\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(y, y_pred_all, s=0.75)\n",
    "plt.ylabel(\"Predicted\")\n",
    "plt.xlabel(\"Observed\")\n",
    "plt.plot([min(y), max(y)], [min(y), max(y)], color=\"red\", linestyle='--', linewidth=2)  \n",
    "plt.title(\"Observed vs Predicted\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a polynomial regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create a pipeline with polynomial features and a linear regression model\n",
    "degree = 2  \n",
    "poly_model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "\n",
    "# Fit the model on the training data\n",
    "poly_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_poly = poly_model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared for the polynomial model\n",
    "r_squared_poly = poly_model.score(X_test, y_test)\n",
    "print(f'R-squared (polynomial): {r_squared_poly}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create a Random Forest Regressor model\n",
    "rf_model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared for the Random Forest model\n",
    "r_squared_rf = rf_model.score(X_test, y_test)\n",
    "print(f'R-squared (Random Forest): {r_squared_rf}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a gradient boosting model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Create a Gradient Boosting Regressor model\n",
    "gb_model = GradientBoostingRegressor(random_state=42)\n",
    "\n",
    "# Fit the model on the training data\n",
    "gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_gb = gb_model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared for the Gradient Boosting model\n",
    "r_squared_gb = gb_model.score(X_test, y_test)\n",
    "print(f'R-squared (Gradient Boosting): {r_squared_gb}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the columns to verify their existence\n",
    "print(\"Columns before dropping:\", w_pickups.columns)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "w_pickups = w_pickups.drop(['name', 'icon', 'stations', 'date'], axis=1, errors='ignore')\n",
    "\n",
    "# Print the columns after dropping\n",
    "print(\"Columns after dropping:\", w_pickups.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = w_pickups[['temp', 'precip', 'lag_1hr', 'lag_24hr', 'lag_168hr']]\n",
    "target = w_pickups['pickup_counts']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Check for Nvidia GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "\n",
    "# Create a Neural Network model with 3 hidden layers\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)  \n",
    "        self.fc3 = nn.Linear(hidden_size, 1)  \n",
    "\n",
    "    def forward(self, X):\n",
    "        out = self.fc1(X)\n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "\n",
    "        return out\n",
    "    \n",
    "# Define the model\n",
    "model = NeuralNet(8, 64).to(device)\n",
    "\n",
    "# Define the loss function and the optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Convert the data to PyTorch tensors and send them to device\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32).to(device)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).to(device)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to(device)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to(device)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "start_time = time()\n",
    "epochs = 200\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "rmse_list = []\n",
    "for i in range(epochs):\n",
    "    y_pred = model(X_train_tensor)\n",
    "    loss = torch.sqrt(criterion(y_pred, y_train_tensor))\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        y_pred = model(X_test_tensor)\n",
    "        loss = torch.sqrt(criterion(y_pred, y_test_tensor))\n",
    "        test_losses.append(loss.item())\n",
    "        rmse = np.sqrt(mean_squared_error(y_test_tensor.cpu().detach().numpy(), y_pred.cpu().detach().numpy()))\n",
    "        rmse_list.append(rmse)\n",
    "\n",
    "    if i % 100 == 0:\n",
    "        print(f'Epoch {i} train loss: {loss.item()}')\n",
    "        print(f'Progress: {(i + 1) / epochs * 100:.2f}%')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train loss')\n",
    "plt.plot(test_losses, label='Test loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(rmse_list, label='RMSE')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('RMSE')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join('plots', 'nn_train_test_loss.png'))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred.cpu().detach().numpy(), label='Predicted')\n",
    "plt.plot(y_test_tensor.cpu().detach().numpy(), label='Actual', alpha=0.5)\n",
    "plt.xlabel('Test samples')\n",
    "plt.ylabel('Pickup counts')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join('plots', 'nn_predictions.png'))\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scientific",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
